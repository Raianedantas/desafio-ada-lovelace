# Projeto - Data Wrangling & Pipeline em Python

## Etapa A - ExploraÃ§Ã£o & ExtraÃ§Ã£o

###  DescriÃ§Ã£o
ExploraÃ§Ã£o e extraÃ§Ã£o de dados brutos do CSV de vendas para preparaÃ§Ã£o do pipeline ETL.

Este Ã© um desafio da equipe SQUAD ADA LOVELACE  referente ao Bootcamp Business Intelligence 2025 Desafio Data Wrangling e Pipelines em Python  o qual o objetivo Ã© a implementaÃ§Ã£o de um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) 
para limpar e processar dados de vendas de uma startupNesta etapa, o foco Ã© compreender o dataset, identificar inconsistÃªncias e preparar a funÃ§Ã£o `extract()` para ser usada nas prÃ³ximas fases (TransformaÃ§Ã£o e Carga). 

---

### Estrutura do Projeto
- `exploracao.py` = exploracao.py: Script dedicado Ã  AnÃ¡lise ExploratÃ³ria de Dados (EDA). Ã‰ usado para carregar, investigar, fatiar e filtrar os dados brutos para entender a fundo os problemas de qualidade.

- `pipeline.py` = O orquestrador principal do pipeline ETL. Este script contÃ©m a estrutura do processo (E-T-L) e a implementaÃ§Ã£o da funÃ§Ã£o extract(), responsÃ¡vel por carregar os dados brutos com logs de execuÃ§Ã£o.

- `README.md` = O 'manual de instruÃ§Ãµes' principal do projeto (este prÃ³prio arquivo!). Ele explica o objetivo, lista os entregÃ¡veis de cada Squad e, o mais importante, ensina como instalar as dependÃªncias (requirements.txt) e rodar os scripts.

- `report_exploracao.md` = O relatÃ³rio de anÃ¡lise (entregÃ¡vel da exploracao.py). Este documento resume as descobertas da exploraÃ§Ã£o, quantifica problemas (nulos, formatos de data, texto em quantidade, etc.) e serve como o principal guia para o Squad B iniciar as transformaÃ§Ãµes.

- `requirements.txt` = bibliotecas necessÃ¡rias para rodar o projeto Estrutura do Projeto

---

###  ExecuÃ§Ã£o Local (Terminal ou VS Code)

```bash
pip install -r requirements.txt
python exploracao.py
```

##  Como Rodar o Projeto

Siga estas etapas para configurar o ambiente e executar os scripts.

1.  **Clone o RepositÃ³rio**
    Baixe os arquivos do projeto para sua mÃ¡quina local.

2.  **Crie um Ambiente Virtual**
    No terminal, dentro da pasta do projeto, crie um ambiente isolado:

    ```bash
    python -m venv venv
    ```

3.  **Ative o Ambiente Virtual**

      * No Windows:
        ```bash
        .\venv\Scripts\activate
        ```
      * No macOS/Linux:
        ```bash
        source venv/bin/activate
        ```

4.  **Instale as DependÃªncias**
    Instale todas as bibliotecas necessÃ¡rias que estÃ£o listadas no `requirements.txt`:

    ```bash
    pip install -r requirements.txt
    ```

5.  **Execute a AnÃ¡lise ExploratÃ³ria (Squad A)**
    Para rodar o script de exploraÃ§Ã£o e ver a anÃ¡lise dos dados brutos:

    ```bash
    python exploracao.py
    ```

6.  **Execute o Pipeline Principal**
    Para rodar o pipeline de ETL completo (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga):

    ```bash
    python pipeline.py
    ```

##### ðŸš€ Nossa Equipe e DivisÃ£o de Tarefas

Para organizar nosso fluxo de trabalho, dividimos o projeto em trÃªs Squads com focos de atuaÃ§Ã£o distintos, baseados na ata de alinhamento do projeto. Cada squad teve autonomia para desenvolver suas soluÃ§Ãµes.

---

> ### ðŸ‘©â€ðŸ’» SQUAD A â€“ ExploraÃ§Ã£o & ExtraÃ§Ã£o
>
> **Foco:** Preparar o ambiente, analisar os dados brutos, identificar problemas e construir a primeira etapa (`Extract`) do pipeline.
>
> **Membros:**
> * [RAINE DANTAS](https://github.com/Raianedantas)
> * [CAROLINA FREIRE](https://github.com/carolfsfreire)
>
> **AtribuiÃ§Ãµes:**
> * **A1 (FÃ¡cil):** ConfiguraÃ§Ã£o do ambiente, repositÃ³rio, `README` inicial e `requirements.txt`.
> * **A2 (FÃ¡cil â†’ MÃ©dio):** AnÃ¡lise exploratÃ³ria de dados (EDA), filtros e ordenaÃ§Ã£o.
> * **A3 (MÃ©dio):** CriaÃ§Ã£o da funÃ§Ã£o `extract()` com logs e da estrutura base do pipeline.
>
> **EntregÃ¡veis:**
> * `exploracao.py`
> * `report_exploracao.md`
> * `pipeline.py` (com a funÃ§Ã£o `extract()`)

---

> ### ðŸ› ï¸ SQUAD B â€“ TransformaÃ§Ãµes
>
> **Foco:** Limpar, tratar e enriquecer os dados. Esta Ã© a etapa (`Transform`) onde a "mÃ¡gica" da limpeza acontece.
>
> **Membros:**
> * [GABRIELLY LIMA](https://github.com/gabrielly-slima/gabrielly-slima)
> * [ALINE NASCIMENTO](https://github.com/alinelimx)
> * [HELENA NOCERA]()
>
> **AtribuiÃ§Ãµes:**
> * **B1 (MÃ©dio):** FunÃ§Ãµes para padronizar datas (`padroniza_data`) e preencher valores nulos (`preenche_nulos`).
> * **B2 (MÃ©dio):** Tratamento de preÃ§os negativos, criaÃ§Ã£o da coluna `valor_total` e polÃ­tica de log de dados descartados.
> * **B3 (DifÃ­cil):** ConversÃ£o da coluna `quantidade` (texto para int) e criaÃ§Ã£o de testes unitÃ¡rios.
>
> **EntregÃ¡veis:**
> * `transform.py`
> * `tests/test_transform.py`

---

> ### ðŸ—ƒï¸ SQUAD C â€“ Carga, Modelo & Qualidade
>
> **Foco:** Carregar os dados limpos no banco de dados (`Load`), modelar as tabelas e garantir a qualidade final dos dados.
>
> **Membros:**
> * [CAMILE SANTANA](https://github.com/ichcamile)
> * [SOPHIA PERAZA](https://github.com/sopbit)
> * [ANA CLARA RODRIGUES](https://github.com/DevAnaClara)
>
> **AtribuiÃ§Ãµes:**
> * **C1 (MÃ©dio):** FunÃ§Ã£o `load(df)` para salvar os dados na tabela principal `tb_vendas`.
> * **C2 (DifÃ­cil):** NormalizaÃ§Ã£o da `tb_clientes`, criaÃ§Ã£o de Chave Estrangeira (FK) e definiÃ§Ã£o da ordem de carga.
> * **C3 (DifÃ­cil):** ImplementaÃ§Ã£o de validaÃ§Ãµes (ex: Pandera), logging avanÃ§ado e script SQL para consulta (`total_por_categoria.sql`).
>
> **EntregÃ¡veis:**
> * `load.py`
> * `schema.sql`
> * `consultas/total_por_categoria.sql`
